import requests
import time
import re
import json
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

# ---------------------------
# é€šç”¨å·¥å…·
# ---------------------------

def clean_title(text: str) -> str:
    text = text.strip()
    text = text.split("\n")[0]
    text = text.split("/")[0]
    text = text.replace("[å¯æ’­æ”¾]", "")
    return text.strip()


def extract_douban_id(url: str) -> str | None:
    m = re.search(r"/subject/(\d+)/", url)
    return m.group(1) if m else None


def extract_chinese_name(name: str) -> str | None:
    parts = re.findall(r"[\u4e00-\u9fff]+", name)
    if not parts:
        return None
    return "".join(parts)


# ---------------------------
# è¯¦æƒ…é¡µï¼ˆå…¬å…±ä¿¡æ¯ï¼‰
# ---------------------------

def fetch_detail(url: str):
    resp = requests.get(url, headers=HEADERS, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    directors = []
    actors = []
    genres = []
    release_date = None
    douban_rating = None

    ld_json = soup.find("script", type="application/ld+json")
    if ld_json:
        try:
            data = json.loads(ld_json.string)

            # ğŸ¬ å¯¼æ¼”
            raw_director = data.get("director")
            if isinstance(raw_director, dict):
                name = extract_chinese_name(raw_director.get("name", ""))
                if name:
                    directors.append(name)
            elif isinstance(raw_director, list):
                for d in raw_director:
                    name = extract_chinese_name(d.get("name", ""))
                    if name:
                        directors.append(name)

            # ğŸ­ ä¸»æ¼”ï¼ˆåªä¸­æ–‡ï¼‰
            for a in data.get("actor", []):
                cn = extract_chinese_name(a.get("name", ""))
                if cn:
                    actors.append(cn)

            # ğŸ ç±»å‹
            genres = data.get("genre", []) or []

            # ğŸ“… ä¸Šæ˜ æ—¥æœŸ
            release_date = data.get("datePublished")

            # â­ è±†ç“£è¯„åˆ†
            if "aggregateRating" in data:
                douban_rating = float(
                    data["aggregateRating"]["ratingValue"]
                )

        except Exception as e:
            print("âš ï¸ JSON-LD è§£æå¤±è´¥:", e)

    return {
        "director": list(dict.fromkeys(directors)),
        "actors": list(dict.fromkeys(actors))[:5],
        "genres": genres,
        "release_date": release_date,
        "douban_rating": douban_rating,
    }


# ---------------------------
# ç”¨æˆ·å…¨éƒ¨å½±è§†ï¼ˆå«è¯„åˆ†æ—¥æœŸï¼‰
# ---------------------------

def fetch_all_movies(douban_user):
    statuses = ["collect", "wish"]

    for status in statuses:
        start = 0
        empty_pages = 0

        while True:
            print(f"â³ æŠ“å–è±†ç“£ {status} start={start}")

            url = f"https://movie.douban.com/people/{douban_user}/{status}"
            params = {
                "start": start,
                "sort": "time",
                "rating": "all",
                "filter": "all",
                "mode": "grid"
            }

            resp = requests.get(url, headers=HEADERS, params=params, timeout=10)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select(".item")

            if not items:
                empty_pages += 1
                if empty_pages >= 2:
                    break
            else:
                empty_pages = 0

            for item in items:
                link_el = item.select_one(".info a")
                if not link_el:
                    continue

                title = clean_title(link_el.text)
                detail_url = link_el["href"]
                douban_id = extract_douban_id(detail_url)

                # â­ è¯„åˆ†æ—¥æœŸï¼ˆåªåœ¨â€œçœ‹è¿‡â€é‡Œæœ‰ï¼‰
                rating_date = None
                if status == "collect":
                    date_el = item.select_one(".date")
                    if date_el:
                        rating_date = date_el.text.strip()

                detail = fetch_detail(detail_url)

                yield {
                    "douban_id": douban_id,
                    "title": title,
                    "status": "çœ‹è¿‡" if status == "collect" else "æƒ³çœ‹",
                    "rating_date": rating_date,
                    **detail
                }

                time.sleep(1)

            start += 15
            time.sleep(2)
